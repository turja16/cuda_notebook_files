{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc6e94-c59b-47ce-bf9b-874164bef4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "struct timespec getDiffTime(struct timespec *startTime, struct timespec *endTime)\n",
    "{\n",
    "\tstruct timespec diff;                                                                         \n",
    "\tdiff.tv_sec = endTime->tv_sec - startTime->tv_sec;                                            \n",
    "\tdiff.tv_nsec = endTime->tv_nsec - startTime->tv_nsec;                                         \n",
    "\tif (diff.tv_nsec < 0) {                                                                       \n",
    "\t\tdiff.tv_nsec += 1.0e9;                                                                \n",
    "\t\tdiff.tv_sec--;                                                                        \n",
    "\t}                                                                                             \n",
    "\treturn diff;                                                                                  \n",
    "} \n",
    "\n",
    "// Uncomment the following statement to show the debug information\n",
    "//#define DEBUG 1\n",
    "#ifdef DEBUG\n",
    "#define DEBUG_PRINT(fmt, args...) printf(fmt, ## args)\n",
    "#else\n",
    "#define DEBUG_PRINT(...) \n",
    "#endif\n",
    "\n",
    "#define BLOCK_SIZE 32\n",
    "#define COARSE_FACTOR 1\n",
    "\n",
    "/*\n",
    " * Figure 5.13: A simple sum reduction kernel.\n",
    " * The sum reduction kernel function running in a single block without requirements on control divergences\n",
    " * The number of threads is the number n of elements in the vector.\n",
    " */\n",
    "__global__ void sumRedKernel0(float* d_vec, unsigned int n)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tds_partialSum[t] = d_vec[t];\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tfor (unsigned int stride = 1; stride < n; stride *= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t % (2 * stride) == 0 && t + stride < n) {\n",
    "\t\t\tds_partialSum[t] += ds_partialSum[t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t, ds_partialSum[t]);\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t + stride, ds_partialSum[t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[0] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/* \n",
    " * Figure 5.15: A kernel with fewer thread divergence.\n",
    " * The sum reduction kernel function running in a single block with fewer control divergences\n",
    " * The number of threads is the minimum power of 2 not smaller than the number n of elements in the vector.\n",
    " * Increase the vector size to be 2 raised to the power of an positive integer by padding with 0s\n",
    " */\n",
    "__global__ void sumRedKernel1(float* d_vec, unsigned int n)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tif (t < n)\n",
    "\t\tds_partialSum[t] = d_vec[t];\n",
    "\telse\n",
    "\t\tds_partialSum[t] = 0.0;    // padding with 0s\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tfor (unsigned int stride = blockDim.x / 2; stride >= 1; stride /= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t < stride) {\n",
    "\t\t\tds_partialSum[t] += ds_partialSum[t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t, ds_partialSum[t]);\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t + stride, ds_partialSum[t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[0] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/* \n",
    " * Exercise 5.1: Modify the kernel in Figure 5.13 to eliminate the waste of half of the threads in each block.\n",
    " * The sum reduction kernel function running in a single block without requirements on control divergences\n",
    " */\n",
    "__global__ void sumRedKernel2(float* d_vec, unsigned int n)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tds_partialSum[t] = d_vec[2 * t];\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tif (2 * t + 1 < n)\n",
    "\t\tds_partialSum[t] += d_vec[2 * t + 1];\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tfor (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t % stride == 0 && 2 * t + stride < blockDim.x) {\n",
    "\t\t\tds_partialSum[2 * t] += ds_partialSum[2 * t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, 2 * t, ds_partialSum[2 * t]);\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, 2 * t + stride, ds_partialSum[2 * t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[0] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/*\n",
    " * Exercise 5.1: Modify the kernel in Figure 5.15 to eliminate the waste of half of the threads in each block.\n",
    " * The sum reduction kernel function running in a single block with fewer control divergences\n",
    " * Increase the vector size to be 2 raised to the power of an positive integer by padding with 0s\n",
    " */\n",
    "__global__ void sumRedKernel3(float* d_vec, unsigned int n)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tds_partialSum[t] = d_vec[t];\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tif (blockDim.x + t < n)\n",
    "\t\tds_partialSum[t] += d_vec[blockDim.x + t];\n",
    "\tDEBUG_PRINT(\"ds_partialSum[%d] = %.1f\\n\", t, ds_partialSum[t]);\n",
    "\tfor (unsigned int stride = blockDim.x / 2; stride >= 1; stride /= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t < stride) {\n",
    "\t\t\tds_partialSum[t] += ds_partialSum[t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t, ds_partialSum[t]);\n",
    "\t\t\tDEBUG_PRINT(\"stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstride, t + stride, ds_partialSum[t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[0] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/*\n",
    " * Exercise 5.3: \n",
    " * The scalable sum reduction kernel function without requirements on control divergences\n",
    " * 1) Add the statements that load a section of the input array from global memory to shared memory.\n",
    " * 2) Use blockIdx.x to allow multiple blocks to work on different sections of the input array. \n",
    " * 3) Write the reduction value for the section to a location according to the blockIdx.x, \n",
    " *    so that all blocks will deposit their section reduction value to the lower part of the input array in global memory.\n",
    " */\n",
    "__global__ void sumRedKernel4(float* d_vec, unsigned int n, unsigned int step)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int s = blockIdx.x * (2 * blockDim.x);\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tif (s + 2 * t < n) {\n",
    "\t\tds_partialSum[t] = d_vec[step * (s + 2 * t)];\n",
    "\t\tDEBUG_PRINT(\"step = %d: d_vec[%d] = %.1f\\n\", step, step * (s + 2 * t), \n",
    "\t\t\t\td_vec[step * (s + 2 * t)]);\n",
    "\t}\n",
    "\tif (s + 2 * t + 1 < n) {\n",
    "\t\tds_partialSum[t] += d_vec[step * (s + 2 * t + 1)];\n",
    "\t\tDEBUG_PRINT(\"step = %d: d_vec[%d] = %.1f\\n\", step, step * (s + 2 * t + 1), \n",
    "\t\t\t\td_vec[step * (s + 2 * t + 1)]);\n",
    "\t}\n",
    "\tunsigned int num;\n",
    "\tif (n - s >= 2 * blockDim.x)\n",
    "\t\tnum = blockDim.x;\n",
    "\telse if ((n - s) % 2 == 0)\n",
    "\t\tnum = (n - s) / 2;\n",
    "\telse\n",
    "\t\tnum = (n - s) / 2 + 1;\n",
    "\tfor (unsigned int stride = 1; stride < num; stride *= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t % stride == 0 && 2 * t + stride < num) {\n",
    "\t\t\tds_partialSum[2 * t] += ds_partialSum[2 * t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"step = %d, stride = %d: d_vec[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstep, stride, step * (s + 2 * t), ds_partialSum[2 * t]);\n",
    "\t\t\tDEBUG_PRINT(\"step = %d, stride = %d: d_vec[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstep, stride, step * (s + 2 * t + stride), \n",
    "\t\t\t\t\tds_partialSum[2 * t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[step * s] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/*\n",
    " * Exercise 5.3: \n",
    " * The scalable sum reduction kernel function with fewer control divergences using padding with 0s\n",
    " * 1) Add the statements that load a section of the input array from global memory to shared memory.\n",
    " * 2) Use blockIdx.x to allow multiple blocks to work on different sections of the input array. \n",
    " * 3) Write the reduction value for the section to a location according to the blockIdx.x, \n",
    " *    so that all blocks will deposit their section reduction value to the lower part of the input array in global memory.\n",
    " */\n",
    "__global__ void sumRedKernel5(float* d_vec, unsigned int n, unsigned int step)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int s = blockIdx.x * (2 * blockDim.x);\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tif (s + t < n)\n",
    "\t\tds_partialSum[t] = d_vec[step * (s + t)];\n",
    "\telse\n",
    "\t\tds_partialSum[t] = 0.0;\n",
    "\tif (s + blockDim.x + t < n)\n",
    "\t\tds_partialSum[t] += d_vec[step * (s + blockDim.x + t)];\n",
    "\tDEBUG_PRINT(\"step = %d: d_vec[%d] = %.1f\\n\", step, step * (s + t), d_vec[step * (s + t)]);\n",
    "\tDEBUG_PRINT(\"step = %d: d_vec[%d] = %.1f\\n\", step, step * (s + blockDim.x + t), d_vec[step * (s + blockDim.x + t)]);\n",
    "\tfor (unsigned int stride = blockDim.x / 2; stride >= 1; stride /= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t < stride) {\n",
    "\t\t\tds_partialSum[t] += ds_partialSum[t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"step = %d, stride = %d: d_vec[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstep, stride, step * (s + t), ds_partialSum[t]);\n",
    "\t\t\tDEBUG_PRINT(\"step = %d, stride = %d: d_vec[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tstep, stride, step * (s + t + stride), ds_partialSum[t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0)\n",
    "\t\td_vec[step * s] = ds_partialSum[0];\n",
    "}\n",
    "\n",
    "/*\n",
    " * Figure 10.15: A segmented multi-block sum reduction kernel using atomic operations and thread coarsening.\n",
    " */\n",
    "__global__ void sumRedKernel6(float* d_vec, float* d_sum, unsigned int n)\n",
    "{\n",
    "\textern __shared__ float ds_partialSum[];\n",
    "\tunsigned int s = blockIdx.x * (2 * blockDim.x) * COARSE_FACTOR;\n",
    "\tunsigned int t = threadIdx.x;\n",
    "\tds_partialSum[t] = 0.0;\n",
    "\tfor (int tileIdx = 0; tileIdx < 2 * COARSE_FACTOR && s + tileIdx * blockDim.x + t < n; tileIdx++) {\n",
    "\t\tds_partialSum[t] += d_vec[s + tileIdx * blockDim.x + t];\n",
    "\t}\n",
    "\tDEBUG_PRINT(\"block = %d: ds_partialSum[%d] = %.1f\\n\", blockIdx.x, t, ds_partialSum[t]);\n",
    "\tfor (unsigned int stride = blockDim.x / 2; stride >= 1; stride /= 2) {\n",
    "\t\t__syncthreads();\n",
    "\t\tif (t < stride) {\n",
    "\t\t\tds_partialSum[t] += ds_partialSum[t + stride];\n",
    "\t\t\tDEBUG_PRINT(\"block = %d, stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tblockIdx.x, stride, t, ds_partialSum[t]);\n",
    "\t\t\tDEBUG_PRINT(\"block = %d, stride = %d: ds_partialSum[%d] = %.1f\\n\", \n",
    "\t\t\t\t\tblockIdx.x, stride, t + stride, ds_partialSum[t + stride]);\n",
    "\t\t}\n",
    "\t}\n",
    "\tif (t == 0) {\n",
    "\t\tDEBUG_PRINT(\"block = %d: d_sum = %.1f\\n\", blockIdx.x, *d_sum);\n",
    "\t\tatomicAdd(d_sum, ds_partialSum[0]);\n",
    "\t\tDEBUG_PRINT(\"block = %d: ds_partialSum[0] = %.1f, d_sum = %.1f\\n\", blockIdx.x, ds_partialSum[0], *d_sum);\n",
    "\t}\n",
    "}\n",
    "\n",
    "float sumRed(float* h_vec, unsigned int n, unsigned int version)\n",
    "{\n",
    "\tunsigned int size = n * sizeof(float);\n",
    "\tfloat *d_vec, *d_sum;\n",
    "\tfloat h_sum = 0.0;\n",
    "\n",
    "\tcudaMalloc((void **) &d_vec, size);\n",
    "\tcudaMalloc((void **) &d_sum, sizeof(float));\n",
    "\n",
    "\tcudaMemcpy(d_vec, h_vec, size, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(d_sum, &h_sum, sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "\tunsigned int blockSize = BLOCK_SIZE, gridSize, sharedMemSize, iter = 0, step = 1;\n",
    "\tswitch(version) {\n",
    "\t\tcase 0:\n",
    "\t\t\tgridSize = 1;\n",
    "\t\t\tblockSize = n;\n",
    "\t\t\tsharedMemSize = n * sizeof(float); \n",
    "\t\t\tsumRedKernel0<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 1:\n",
    "\t\t\tgridSize = 1;\n",
    "\t\t\tblockSize = 1 << ((int) ceil(log2(n)));    // the minimum power of 2 not smaller than n\n",
    "\t\t\tsharedMemSize = blockSize * sizeof(float); \n",
    "\t\t\tsumRedKernel1<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 2:\n",
    "\t\t\tgridSize = 1;\n",
    "\t\t\tblockSize = ceil(n / 2.0);\n",
    "\t\t\tsharedMemSize = blockSize * sizeof(float); \n",
    "\t\t\tsumRedKernel2<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 3:\n",
    "\t\t\tgridSize = 1;\n",
    "\t\t\tblockSize = 1 << ((int) ceil(log2(n)) - 1);\n",
    "\t\t\tsharedMemSize = blockSize * sizeof(float); \n",
    "\t\t\tsumRedKernel3<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 4:\n",
    "\t\t\t/*\n",
    "\t\t\t * Exercise 5.4: \n",
    "\t\t\t * Use a loop to repeatedly invoke the kernel function sumRedKernel4\n",
    "\t\t\t * with adjusted execution configuration parameter values so that\n",
    "\t\t\t * the reduction result for the input array will eventually be produced.\n",
    "\t\t\t */\n",
    "\t\t\tdo {\n",
    "\t\t\t\tif (BLOCK_SIZE < ceil(n / 2.0)) {\n",
    "\t\t\t\t\tblockSize = BLOCK_SIZE;\n",
    "\t\t\t\t\tgridSize = ceil(n / (2.0 * blockSize));\n",
    "\t\t\t\t} else {\n",
    "\t\t\t\t\tgridSize = 1;\n",
    "\t\t\t\t\tblockSize = ceil(n / 2.0);\n",
    "\t\t\t\t}\n",
    "\t\t\t\tsharedMemSize = blockSize * sizeof(float);\n",
    "\t\t\t\tstep = (unsigned int) pow(2 * BLOCK_SIZE, iter);\n",
    "\t\t\t\tsumRedKernel4<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n, step);\n",
    "\t\t\t\titer++;\n",
    "\t\t\t\tn = gridSize;\n",
    "\t\t\t} while (n > 1);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 5:\n",
    "\t\tdefault:\n",
    "\t\t\t/*\n",
    "\t\t\t * Exercise 5.4: \n",
    "\t\t\t * Use a loop to repeatedly invoke the kernel function sumRedKernel5\n",
    "\t\t\t * with adjusted execution configuration parameter values so that\n",
    "\t\t\t * the reduction result for the input array will eventually be produced.\n",
    "\t\t\t */\n",
    "\t\t\tdo {\n",
    "\t\t\t\tif (BLOCK_SIZE < ceil(n / 2.0)) {\n",
    "\t\t\t\t\tblockSize = 1 << (int) ceil(log2(BLOCK_SIZE));\n",
    "\t\t\t\t\tgridSize = ceil(n / (2.0 * blockSize));\n",
    "\t\t\t\t\tstep = (unsigned int) pow(2 * blockSize, iter);\n",
    "\t\t\t\t} else {\n",
    "\t\t\t\t\tstep = (unsigned int) pow(2 * blockSize, iter);\n",
    "\t\t\t\t\tgridSize = 1;\n",
    "\t\t\t\t\tblockSize = 1 << ((int) ceil(log2(n)) - 1);\n",
    "\t\t\t\t}\n",
    "\t\t\t\tsharedMemSize = blockSize * sizeof(float); \n",
    "\t\t\t\tsumRedKernel5<<<gridSize, blockSize, sharedMemSize>>>(d_vec, n, step);\n",
    "\t\t\t\titer++;\n",
    "\t\t\t\tn = gridSize;\n",
    "\t\t\t} while (n > 1);\n",
    "\t\tcase 6:\n",
    "\t\t\tblockSize = 1 << (int) ceil(log2(BLOCK_SIZE));\n",
    "\t\t\tgridSize = ceil(n / (2.0 * blockSize * COARSE_FACTOR));\n",
    "\t\t\tsharedMemSize = blockSize * sizeof(float); \n",
    "\t\t\tsumRedKernel6<<<gridSize, blockSize, sharedMemSize>>>(d_vec, d_sum, n);\n",
    "\t\t\tbreak;\n",
    "\t}\n",
    "\n",
    "\tif (version == 6)\n",
    "\t\tcudaMemcpy(&h_sum, d_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\telse\n",
    "\t\tcudaMemcpy(&h_sum, d_vec, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "\tcudaFree(d_vec);\n",
    "\tcudaFree(d_sum);\n",
    "\treturn h_sum;\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\tint version = 0;\n",
    "\tif (argc > 1) {\n",
    "\t\tversion = atoi(argv[1]);\n",
    "\t\tprintf(\"Kernel version = %d\\n\", version);\n",
    "\t}\n",
    "\n",
    "\tprintf(\"Enter the number of elements to be summed up: \");\n",
    "\tunsigned int n;\n",
    "\tscanf(\"%d\", &n);\n",
    "\n",
    "\tfloat *h_vec;\n",
    "\th_vec = (float *) malloc(n * sizeof(float));\n",
    "\n",
    "\tfor(int i = 0; i < n; i++)\n",
    "\t\th_vec[i] = 2.0;\n",
    "\n",
    "\tstruct timespec startTime;\n",
    "\tclock_gettime(CLOCK_REALTIME, &startTime);\n",
    "\tfloat sum = sumRed(h_vec, n, version);\n",
    "\tstruct timespec endTime;\n",
    "\tclock_gettime(CLOCK_REALTIME, &endTime);\n",
    "\n",
    "\tprintf(\"sum = %.1f\\n\", sum);\n",
    "\tstruct timespec diffTime = getDiffTime(&startTime, &endTime);\n",
    "\tprintf(\"Execution time: %ld s and %ld us.\\n\", diffTime.tv_sec, diffTime.tv_nsec / (long) 1000);\n",
    "\n",
    "\tfree(h_vec);\n",
    "\treturn 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-C",
   "language": "Cuda-C",
   "name": "cuda-c"
  },
  "language_info": {
   "file_extension": ".cu",
   "mimetype": "text/plain",
   "name": "cuda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
