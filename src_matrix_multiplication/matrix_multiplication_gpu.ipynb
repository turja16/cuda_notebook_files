{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc6e94-c59b-47ce-bf9b-874164bef4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <math.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "struct timespec getDiffTime(struct timespec *startTime, struct timespec *endTime)\n",
    "{\n",
    "\tstruct timespec diff;                                                                         \n",
    "\tdiff.tv_sec = endTime->tv_sec - startTime->tv_sec;                                            \n",
    "\tdiff.tv_nsec = endTime->tv_nsec - startTime->tv_nsec;                                         \n",
    "\tif (diff.tv_nsec < 0) {                                                                       \n",
    "\t\tdiff.tv_nsec += 1.0e9;                                                                \n",
    "\t\tdiff.tv_sec--;                                                                        \n",
    "\t}                                                                                             \n",
    "\treturn diff;                                                                                  \n",
    "} \n",
    "\n",
    "// Comment the following statement to hide the printed elements in all matrices\n",
    "//#define OUTPUT 1\n",
    "#ifdef OUTPUT\n",
    "#define FUNC_PRINT(fmt, args...) printf(fmt, ## args)\n",
    "#else\n",
    "#define FUNC_PRINT(...) \n",
    "#endif\n",
    "\n",
    "#define TILE_WIDTH 32\n",
    "\n",
    "struct Dim {\n",
    "\tunsigned int nr;\t// the number of rows in M\n",
    "\tunsigned int ne;\t// the number of columns in M and the number of rows in N\n",
    "\tunsigned int nc;\t// the number of columns in N\n",
    "};\n",
    "\n",
    "// Figure 4.3: A simple matrix multiplication kernel using one thread to compute one P element.\n",
    "// Each thread produces one output matrix element.\n",
    "__global__ void matMulKernel0(float* d_P, float* d_M, float* d_N, Dim dim)\n",
    "{\n",
    "\t// Calculate the column index of the element in d_P\n",
    "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\t// Calculate the row index of the element in d_P\n",
    "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "\tif (row < dim.nr && col < dim.nc) {\n",
    "\t\tfloat dotProc = 0.0;\n",
    "\t\tfor (int k = 0; k < dim.ne; k++) {\n",
    "\t\t\tdotProc += d_M[row * dim.ne + k] * d_N[k * dim.nc + col];\n",
    "\t\t}\n",
    "\t\td_P[row * dim.nc + col] = dotProc;\n",
    "\t}\n",
    "}\n",
    "\n",
    "/*\n",
    " * Figures 4.16 and 4.20: A tiled matrix multiplication kernel \n",
    " * with square tiles\n",
    " * with boundary condition checks \n",
    " * using shared memory with a fixed size.\n",
    " */\n",
    "__global__ void matMulKernel1(float* d_P, float* d_M, float* d_N, Dim dim)\n",
    "{\n",
    "\t__shared__ float ds_M[TILE_WIDTH * TILE_WIDTH];\n",
    "\t__shared__ float ds_N[TILE_WIDTH * TILE_WIDTH];\n",
    "\n",
    "\tint tx = threadIdx.x;\n",
    "\tint ty = threadIdx.y;\n",
    "\n",
    "\t// Identify the row and column of the d_P element to work on\n",
    "\tint col = blockIdx.x * TILE_WIDTH + tx;\n",
    "\tint row = blockIdx.y * TILE_WIDTH + ty;\n",
    "\n",
    "\tfloat dotProc = 0.0;\n",
    "\tint num_phase = ceil(dim.ne / (float) TILE_WIDTH);\n",
    "\n",
    "\t// Loop over the d_M and d_N tiles required to compute d_P element\n",
    "\tfor (int phase = 0; phase < num_phase; phase++) {\n",
    "\t\t// Collaborative loading of d_M and d_N tiles into shared memory\n",
    "\t\tif (row < dim.nr && phase * TILE_WIDTH + tx < dim.ne)\n",
    "\t\t\tds_M[ty * TILE_WIDTH + tx] = d_M[row * dim.ne + phase * TILE_WIDTH + tx];\n",
    "\t\tif (col < dim.nc && phase * TILE_WIDTH + ty < dim.ne)\n",
    "\t\t\tds_N[ty * TILE_WIDTH + tx] = d_N[(phase * TILE_WIDTH + ty) * dim.nc + col];\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\n",
    "\t\tint phase_length = TILE_WIDTH;\n",
    "\t\tif (phase == num_phase - 1)\n",
    "\t\t\tphase_length = dim.ne % TILE_WIDTH;\n",
    "\t\tfor (int k = 0; k < phase_length; k++) {\n",
    "\t\t\tdotProc += ds_M[ty * TILE_WIDTH + k] * ds_N[k * TILE_WIDTH + tx];\n",
    "\t\t}\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\t}\n",
    "\tif (col < dim.nc && row < dim.nr)\n",
    "\t\td_P[row * dim.nc + col] = dotProc;\n",
    "}\n",
    "\n",
    "/*\n",
    " * Figures 4.16 and 4.20: A tiled matrix multiplication kernel \n",
    " * with square tiles\n",
    " * with boundary condition checks \n",
    " * using shared memory with an adjustable size.\n",
    " */\n",
    "__global__ void matMulKernel2(float* d_P, float* d_M, float* d_N, Dim dim, unsigned int tile_width)\n",
    "{\n",
    "\textern __shared__ float ds[];\n",
    "\tfloat *ds_M = ds;\n",
    "\tfloat *ds_N = (float *) &ds_M[tile_width * tile_width];\n",
    "\n",
    "\tint tx = threadIdx.x;\n",
    "\tint ty = threadIdx.y;\n",
    "\n",
    "\t// Identify the row and column of the d_P element to work on\n",
    "\tint col = blockIdx.x * tile_width + tx;\n",
    "\tint row = blockIdx.y * tile_width + ty;\n",
    "\n",
    "\tfloat dotProc = 0.0;\n",
    "\tint num_phase = ceil(dim.ne / (float) tile_width);\n",
    "\n",
    "\t// Loop over the d_M and d_N tiles required to compute d_P element\n",
    "\tfor (int phase = 0; phase < num_phase; phase++) {\n",
    "\t\t// Collaborative loading of d_M and d_N tiles into shared memory\n",
    "\t\tif (row < dim.nr && phase * tile_width + tx < dim.ne)\n",
    "\t\t\tds_M[ty * tile_width + tx] = d_M[row * dim.ne + phase * tile_width + tx];\n",
    "\t\tif (col < dim.nc && phase * tile_width + ty < dim.ne)\n",
    "\t\t\tds_N[ty * tile_width + tx] = d_N[(phase * tile_width + ty) * dim.nc + col];\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\n",
    "\t\tint phase_length = tile_width;\n",
    "\t\tif (phase == num_phase - 1)\n",
    "\t\t\tphase_length = dim.ne % tile_width;\n",
    "\t\tfor (int k = 0; k < phase_length; k++) {\n",
    "\t\t\tdotProc += ds_M[ty * tile_width + k] * ds_N[k * tile_width + tx];\n",
    "\t\t}\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\t}\n",
    "\tif (col < dim.nc && row < dim.nr)\n",
    "\t\td_P[row * dim.nc + col] = dotProc;\n",
    "}\n",
    "\n",
    "/*\n",
    " * Exercise 5.10/Figure 5.17: A tiled matrix multiplication kernel \n",
    " * with rectangular tiles (combining two adjacent horizontal blocks to compute adjacent horizontal tiles)\n",
    " * with boundary condition checks \n",
    " * using shared memory with an adjustable size.\n",
    " */\n",
    "__global__ void matMulKernel3(float* d_P, float* d_M, float* d_N, Dim dim, unsigned int tile_width)\n",
    "{\n",
    "\textern __shared__ float ds[];\n",
    "\tfloat *ds_M = ds;\n",
    "\tfloat *ds_N1 = (float *) &ds_M[tile_width * tile_width];\n",
    "\tfloat *ds_N2 = (float *) &ds_N1[tile_width * tile_width];\n",
    "\n",
    "\tint tx = threadIdx.x;\n",
    "\tint ty = threadIdx.y;\n",
    "\n",
    "\t// Identify the row and column of the d_P element to work on\n",
    "\tint col = blockIdx.x * tile_width * 2 + tx;\n",
    "\tint row = blockIdx.y * tile_width + ty;\n",
    "\n",
    "\tfloat dotProc1 = 0.0, dotProc2 = 0.0;\n",
    "\tint num_phase = ceil(dim.ne / (float) tile_width);\n",
    "\n",
    "\t// Loop over the d_M and d_N tiles required to compute d_P element\n",
    "\tfor (int phase = 0; phase < num_phase; phase++) {\n",
    "\t\t// Collaborative loading of d_M and d_N tiles into shared memory\n",
    "\t\tif (row < dim.nr && phase * tile_width + tx < dim.ne)\n",
    "\t\t\tds_M[ty * tile_width + tx] = d_M[row * dim.ne + phase * tile_width + tx];\n",
    "\t\tif (col < dim.nc && phase * tile_width + ty < dim.ne)\n",
    "\t\t\tds_N1[ty * tile_width + tx] = d_N[(phase * tile_width + ty) * dim.nc + col];\n",
    "\t\tif (col + tile_width < dim.nc && phase * tile_width + ty < dim.ne)\n",
    "\t\t\tds_N2[ty * tile_width + tx] = d_N[(phase * tile_width + ty) * dim.nc + col + tile_width];\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\n",
    "\t\tint phase_length = tile_width;\n",
    "\t\tif (phase == num_phase - 1)\n",
    "\t\t\tphase_length = dim.ne % tile_width;\n",
    "\t\tfor (int k = 0; k < phase_length; k++) {\n",
    "\t\t\tdotProc1 += ds_M[ty * tile_width + k] * ds_N1[k * tile_width + tx];\n",
    "\t\t\tdotProc2 += ds_M[ty * tile_width + k] * ds_N2[k * tile_width + tx];\n",
    "\t\t}\n",
    "\n",
    "\t\t__syncthreads();\n",
    "\t}\n",
    "\tif (col < dim.nc && row < dim.nr)\n",
    "\t\td_P[row * dim.nc + col] = dotProc1;\n",
    "\tif (col + tile_width < dim.nc && row < dim.nr)\n",
    "\t\td_P[row * dim.nc + col + tile_width] = dotProc2;\n",
    "}\n",
    "\n",
    "/*\n",
    " * A host stub function:  \n",
    " * allocating memory for the input and output matrices, \n",
    " * transferring input data to device, \n",
    " * launch the kernel, \n",
    " * transferring the output data to host, \n",
    " * and freeing the device memory for the input and output data.\n",
    " */\n",
    "void matMul(float* h_P, float* h_M, float* h_N, Dim dim, int opt)\n",
    "{\n",
    "\tfloat *d_M, *d_N, *d_P;    // pointers to device copies of M, N, P\n",
    "\tint size_M = dim.nr * dim.ne * sizeof(float);\n",
    "\tint size_N = dim.ne * dim.nc * sizeof(float);\n",
    "\tint size_P = dim.nr * dim.nc * sizeof(float);\n",
    "\n",
    "\t// Allocate device memory space for device copies of M, N, P\n",
    "\tcudaMalloc((void **) &d_M, size_M);\n",
    "\tcudaMalloc((void **) &d_N, size_N);\n",
    "\tcudaMalloc((void **) &d_P, size_P);\n",
    "\n",
    "\t// Copy matrices M and N from host memory to device memory\n",
    "\tcudaMemcpy(d_M, h_M, size_M, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(d_N, h_N, size_N, cudaMemcpyHostToDevice);\n",
    "\n",
    "\t// Launch the kernel function to have the device to perform the actual matrix addition\n",
    "\tdim3 dimGrid(ceil(dim.nc / (float) TILE_WIDTH), ceil(dim.nr / (float) TILE_WIDTH), 1);\n",
    "\tdim3 dimBlock(TILE_WIDTH, TILE_WIDTH, 1);\n",
    "\tcudaDeviceProp prop;\n",
    "\tunsigned int tile_width, sharedMemSize;\n",
    "\tswitch(opt) {\n",
    "\t\tcase 0:\n",
    "\t\t\tmatMulKernel0<<<dimGrid, dimBlock>>>(d_P, d_M, d_N, dim);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 1:\n",
    "\t\t\tmatMulKernel1<<<dimGrid, dimBlock>>>(d_P, d_M, d_N, dim);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 2:\n",
    "\t\t\tcudaGetDeviceProperties(&prop, 0);\n",
    "\t\t\ttile_width = TILE_WIDTH;\n",
    "\t\t\tsharedMemSize = 2 * TILE_WIDTH * TILE_WIDTH * sizeof(float);\n",
    "\t\t\tif (sharedMemSize > prop.sharedMemPerBlock) {\n",
    "\t\t\t\ttile_width = floor(sqrt(prop.sharedMemPerBlock / 2.0 / sizeof(float)));\n",
    "\t\t\t\tsharedMemSize = 2 * tile_width * tile_width * sizeof(float);\n",
    "\t\t\t}\n",
    "\t\t\tdimBlock.x = dimBlock.y = tile_width;\n",
    "\t\t\tdimGrid.x = ceil(dim.nc / (float) tile_width);\n",
    "\t\t\tdimGrid.y = ceil(dim.nr / (float) tile_width);\n",
    "\t\t\tmatMulKernel2<<<dimGrid, dimBlock, sharedMemSize>>>(d_P, d_M, d_N, dim, tile_width);\n",
    "\t\t\tbreak;\n",
    "\t\tcase 3:\n",
    "\t\tdefault:\n",
    "\t\t\tcudaGetDeviceProperties(&prop, 0);\n",
    "\t\t\ttile_width = TILE_WIDTH;\n",
    "\t\t\tsharedMemSize = 3 * TILE_WIDTH * TILE_WIDTH * sizeof(float);\n",
    "\t\t\tif (sharedMemSize > prop.sharedMemPerBlock) {\n",
    "\t\t\t\ttile_width = floor(sqrt(prop.sharedMemPerBlock / 3.0 / sizeof(float)));\n",
    "\t\t\t\tsharedMemSize = 3 * tile_width * tile_width * sizeof(float);\n",
    "\t\t\t}\n",
    "\t\t\tdimBlock.x = dimBlock.y = tile_width;\n",
    "\t\t\tdimGrid.x = ceil(dim.nc / (2.0 * tile_width));\n",
    "\t\t\tdimGrid.y = ceil(dim.nr / (float) tile_width);\n",
    "\t\t\tmatMulKernel3<<<dimGrid, dimBlock, sharedMemSize>>>(d_P, d_M, d_N, dim, tile_width);\n",
    "\t}\n",
    "\n",
    "\t// Copy result matrix P from the device memory to host memory\n",
    "\tcudaMemcpy(h_P, d_P, size_P, cudaMemcpyDeviceToHost);\n",
    "\n",
    "\t// Free device memory for M, N, P\n",
    "\tcudaFree(d_M);\n",
    "\tcudaFree(d_N);\n",
    "\tcudaFree(d_P);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "\tint opt = 0;\n",
    "\tif (argc > 1) {\n",
    "\t\topt = atoi(argv[1]);\n",
    "\t\tprintf(\"Optimization level = %d\\n\", opt);\n",
    "\t}\n",
    "\n",
    "\tstruct Dim dim;\n",
    "\tprintf(\"Enter the number of rows in M: \");\n",
    "\tscanf(\"%d\", &dim.nr);\n",
    "\tprintf(\"Enter the number of columns in M and the number of rows in N: \");\n",
    "\tscanf(\"%d\", &dim.ne);\n",
    "\tprintf(\"Enter the number of columns in N: \");\n",
    "\tscanf(\"%d\", &dim.nc);\n",
    "\n",
    "\tint size_M = dim.nr * dim.ne * sizeof(float);\n",
    "\tint size_N = dim.ne * dim.nc * sizeof(float);\n",
    "\tint size_P = dim.nr * dim.nc * sizeof(float);\n",
    "\n",
    "\t// Memory allocation for h_M, h_N, and h_P\n",
    "\tfloat *h_M = (float *) malloc(size_M);\n",
    "\tfloat *h_N = (float *) malloc(size_N);\n",
    "\tfloat *h_P = (float *) malloc(size_P);\n",
    "\n",
    "\t// Setup input values into each of n elements of h_M and h_N\n",
    "\tFUNC_PRINT(\"Matrix M:\\n\");\n",
    "\tfor(int row = 0; row < dim.nr; row++) {\n",
    "\t\tfor(int col = 0; col < dim.ne; col++) {\n",
    "\t\t\tint offset = row * dim.ne + col;\n",
    "\t\t\th_M[offset] = (float) offset;\n",
    "\t\t\tFUNC_PRINT(\"%.1f\\t\", h_M[offset]);\n",
    "\t\t}\n",
    "\t\tFUNC_PRINT(\"\\n\");\n",
    "\t}\n",
    "\tFUNC_PRINT(\"*\\nMatrix N:\\n\");\n",
    "\tfor(int row = 0; row < dim.ne; row++) {\n",
    "\t\tfor(int col = 0; col < dim.nc; col++) {\n",
    "\t\t\tint offset = row * dim.nc + col;\n",
    "\t\t\th_N[offset] = (float) offset;\n",
    "\t\t\tFUNC_PRINT(\"%.1f\\t\", h_N[offset]);\n",
    "\t\t}\n",
    "\t\tFUNC_PRINT(\"\\n\");\n",
    "\t}\n",
    "\n",
    "\t// Call the host function for matrix multiplication\n",
    "\tstruct timespec startTime;\n",
    "\tclock_gettime(CLOCK_REALTIME, &startTime);\n",
    "\tmatMul(h_P, h_M, h_N, dim, opt);\n",
    "\tstruct timespec endTime;\n",
    "\tclock_gettime(CLOCK_REALTIME, &endTime);\n",
    "\n",
    "\t// Output the results\n",
    "\tFUNC_PRINT(\"=\\nMatrix P:\\n\");\n",
    "\tfor(int row = 0; row < dim.nr; row++) {\n",
    "\t\tfor(int col = 0; col < dim.nc; col++)\n",
    "\t\t\tFUNC_PRINT(\"%.1f\\t\", h_P[row * dim.nc + col]);\n",
    "\t\tFUNC_PRINT(\"\\n\");\n",
    "\t}\n",
    "\n",
    "\tstruct timespec diffTime = getDiffTime(&startTime, &endTime);\n",
    "\tprintf(\"Execution time: %ld s and %ld us.\\n\", diffTime.tv_sec, (long) round(diffTime.tv_nsec / 1000.0));\n",
    "\n",
    "\t// Free host memory for M, N, P\n",
    "\tfree(h_M);\n",
    "\tfree(h_N);\n",
    "\tfree(h_P);\n",
    "\treturn 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-C",
   "language": "Cuda-C",
   "name": "cuda-c"
  },
  "language_info": {
   "file_extension": ".cu",
   "mimetype": "text/plain",
   "name": "cuda"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
